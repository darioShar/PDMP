seed: null

data:
  between_minus_1_1: true
  quantile_cutoff: 0.99999
  data_alpha: 2.0
  dataset: gmm_grid #gmm_grid # sas_grid
  isotropic: true
  n_mixture: 9 # must be square. set to None if not a grid mixture
  dim: 2 # if data_type is 2d and this is one, will just project on first dimension
  normalized: false #true
  nsamples: 100000
  bs: 4096 # batch size
  num_workers: 0 # number of workers for data loading
  std: 0.1
  theta: 0.5
  weights:
  #- 0.05
  #- 0.95
  - 0.01
  - 0.1
  - 0.3
  - 0.2
  - 0.02
  - 0.15
  - 0.02
  - 0.15
  - 0.05

noising_process: 'pdmp' # pdmp, diffusion, None

diffusion:
  alpha: 2.0
  clamp_a: null
  clamp_eps: null
  reverse_steps: 100 # here our diffusion models are trained with the uniforma distribution over [1, diffusion_steps] instead of [0, 1]: thus it is part of the training parameters
  LIM: false
  isotropic: true
  rescale_timesteps: true
  mean_predict: EPSILON
  var_predict: FIXED
  loss_type: LP_EPS_LOSS

pdmp:
  sampler: HMC # ZigZag, BPS, HMC
  time_horizon: 10
  refresh_rate: 1.
  add_losses: 
  #- hyvarinen # usually we choose it by default. Works on reflections. Mandatory for ZigZag.
  #- ml # maximum likelihood
  #- square
  #- kl
  #- logistic
  learn_jump_time: false
  denoiser: false

# needs a placeholder because of the structure of the program
nf:
  placeholder:  # need to define a placeholder


eval:
  data_to_generate: 10000
  batch_size: 4096
  real_data: 10000 # in case of images, number of real images to store and to compare to

  pdmp:
    backward_scheme: splitting # euler
    reverse_steps: 100 # reverse_steps are only part of evaluation, not of the training
    exponent: 2 # might need to remove that for hashes to match afterwards.
    #get_sample_history: false
    #clip_denoised: false
    #new_time_spacing
  # for diffusion
  diffusion:
    reverse_steps: 100
    clip_denoised: false
    ddim: false
    eta: 1.0
    #new_time_spacing

  # needs a placeholder because of the structure of the program
  nf:
    placeholder:

model:
  # this is for 2d ZigZag/diffusion
  mlp:
    act: silu
    dropout_rate: 0.0
    group_norm: true
    nblocks: 8
    nunits: 256
    skip_connection: true
    beta: 0.2
    threshold: 20
    time_emb_size: 32
    time_emb_type: learnable
  
  # this is for the other samplers; normalizing flows
  normalizing_flow:
    transforms: 3
    hidden_width: 256
    hidden_depth: 8
    time_emb_type: learnable
    time_emb_size: 8
    x_emb_size: 8
    x_emb_type: mlp
    model_type: NSF
  
  # use VAE or not
  vae: false
  
  # use a simple normalizing flow for generation
  nf:
    transforms: 3
    hidden_width: 256
    hidden_depth: 8
    model_type: NSF

training:

  pdmp:
    ema_rates: null
    #- 0.9
    grad_clip: null #1.0
    train_type: NORMAL # This is for training conjointly with VAE. assert train_type in ['VAE', 'RATIO', 'NORMAL', 'NORMAL_WITH_VAE']
    train_alternate: false
    exponent: 2.0
  # no need to touch this, default for diffusion
  diffusion:
    ema_rates: null
    #- 0.9
    grad_clip: null #1.0
    loss_monte_carlo: mean # loss to apply on batch of M number of a's. can be mean or median
    lploss: 2.
    monte_carlo_steps: 1 
    monte_carlo_groups: 1
  
  # needs a placeholder because of the structure of the program
  nf:
    placeholder:

optim:
  optimizer: adamw
  schedule: null #steplr, linear
  lr: 0.002
  warmup: 0 #1000
  lr_steps: 300000
  lr_step_size: 400
  lr_gamma: 0.99

run:
  epochs: 1000
  eval_freq: null
  checkpoint_freq: null
  progress: true # print progress bar

additional:
  use_softmax: false
  bin_input_zigzag: false