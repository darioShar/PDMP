seed: null

data:
  channels: 3
  dataset: cifar10
  image_size: 32
  random_flip: true
  #num_classes: 10

noising_process: 'pdmp' # pdmp, diffusion, None

pdmp:
  sampler: HMC # ZigZag, BPS, HMC
  time_horizon: 5
  refresh_rate: 1.
  add_losses: 
  #- hyvarinen
  # - ml
  #- logistic
  learn_jump_time: false

# needs a placeholder because of the structure of the program
nf:
  placeholder:  # need to define a placeholder

diffusion:
  alpha: 2.0
  clamp_a: null
  clamp_eps: null
  reverse_steps: 100 # here our diffusion models are trained with the uniforma distribution over [1, diffusion_steps] instead of [0, 1]: thus it is part of the training parameters
  LIM: false
  isotropic: true
  rescale_timesteps: true
  mean_predict: EPSILON
  var_predict: FIXED
  loss_type: LP_EPS_LOSS


eval:
  data_to_generate: 128
  batch_size: 128
  real_data: 1024 # in case of images, number of real images to store and to compare to

  pdmp:
    backward_scheme: splitting # euler
    reverse_steps: 50 # reverse_steps are only part of evaluation, not of the training
    exponent: 2 # might need to remove that for hashes to match afterwards.
    #clip_denoised: false
    #new_time_spacing
  # for diffusion
  diffusion:
    reverse_steps: 100
    clip_denoised: false
    ddim: false
    eta: 1.0
    #new_time_spacing
  # needs a placeholder because of the structure of the program
  nf:
    placeholder:  # need to define a placeholder

  

model:
  unet:
    model_type: "ddpm"
    attn_resolutions: [2, 4, 16] # maybe [16,]
    channel_mult: [1, 2, 2, 2]
    compute_gamma: false
    dropout: 0.1
    model_channels: 32
    num_heads: 4
    num_res_blocks: 2
    beta: 0.2
    threshold: 20
  # this is for the other samplers; normalizing flows
  normalizing_flow:
#    transforms: 24
#    hidden_width: 1024 # 2048
#    hidden_depth: 4
#    time_emb_size: 32
#    time_emb_type: learnable
#    x_emb_size: 32
#    x_emb_type: mlp # mlp, unet
#  vae: false
    transforms: 9
    hidden_width: 2048 # 2048
    hidden_depth: 4
    time_emb_size: 512
    time_emb_type: learnable
    x_emb_size: 16
    x_emb_type: unet # mlp, unet
    model_type: MAF # NSF, MAF

    model_vae_hidden_features: 64
    model_vae_type: VAE_16 # VAE_1, VAE_16
    model_vae_t_hidden_width: 64
    model_vae_t_emb_size: 64
    model_vae_x_emb_size: 64
  vae: false

  # use a simple normalizing flow / vae for generation
  nf:
    transforms: 8
    hidden_width: 256
    hidden_depth: 8
    model_type: NSF
  


training:
  num_workers: 4 # 2* num_GPU
  batch_size: 64 # batch size
  pdmp:
    ema_rates:
    - 0.9999
    grad_clip: 1.0 #1.0
    train_type: NORMAL # assert train_type in ['VAE', 'RATIO', 'NORMAL', 'NORMAL_WITH_VAE']
    train_alternate: false
    exponent: 2.0
  diffusion:
    ema_rates: 
    - 0.9999
    grad_clip: null #1.0
    loss_monte_carlo: mean # loss to apply on batch of M number of a's. can be mean or median
    lploss: 2.
    monte_carlo_steps: 1 
    monte_carlo_groups: 1 
  # needs a placeholder because of the structure of the program
  nf:
    placeholder:


optim:
  optimizer: adamw
  schedule: null #steplr
  lr: 0.0002
  warmup: 500 #100
  lr_steps: 300000
  lr_step_size: 1000
  lr_gamma: 0.99

run:
  epochs: 400 #10000
  eval_freq: null
  checkpoint_freq: 50
  progress: true # print progress bar


additional:
  use_softmax: false
  bin_input_zigzag: false