seed: null

data:
    dataset: cifar10
    image_size: 32
    channels: 3
    random_flip: true
    num_workers: 12
    #num_classes: 2

eval:
  data_to_generate: 128
  ddim: true
  eval_eta: 0.0
  reduce_timesteps: 10.0
  clip_denoised: true


model:
    #model_type: "ddpm"
    #in_channels: 3
    #out_ch: 3
    attn_resolutions: [16,]
    channel_mult: [1, 2, 2, 2]
    compute_gamma: false
    dropout: 0.1
    model_channels: 128
    num_heads: 4
    num_res_blocks: 2

diffusion:
    alpha: 1.8
    clamp_a: 20
    clamp_eps: 200
    diffusion_steps: 1000
    isotropic: true
    mean_predict: EPSILON
    rescale_timesteps: true
    var_predict: FIXED

training:
    bs: 64
    ema_rates:
    - 0.9999
    grad_clip: 1.0
    loss_monte_carlo: mean # loss to apply on batch of M number of a's. can be mean or median
    loss_type: LP_EPS_LOSS
    lploss: 1.0
    monte_carlo_steps: 1 # for each t, x_0, z_t, number M of different a_t_1, a_t' to generate

optim:
    lr: 0.0001
    lr_steps: 5000000
    optimizer: adamw
    schedule: linear
    warmup: 0

run:
  epochs: 10000 #10000
  eval_freq: 1
  checkpoint_freq: 1
  progress: true # print progress bar